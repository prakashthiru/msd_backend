{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window, Row\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql.functions import rank, row_number\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "import redis\n",
    "import pandas as pd\n",
    "import app_constants\n",
    "import spark_setup\n",
    "import database_setup\n",
    "\n",
    "db = database_setup.DatabaseSetup.db\n",
    "spark = spark_setup.SparkSetup.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session_df = spark.read \\\n",
    "                  .option(\"delimiter\", \",\") \\\n",
    "                  .option(\"inferSchema\", \"true\") \\\n",
    "                  .option(\"header\", spark_setup.SparkSetup.data_headers) \\\n",
    "                  .csv('data/test.csv')\n",
    "\n",
    "clean_df = session_df.drop_duplicates() \\\n",
    "                  .dropna(subset=app_constants.Columns.REQUIRED) \\\n",
    "                  .select(app_constants.Columns.REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_window = Window.partitionBy(clean_df.dateAdded) \\\n",
    "                  .orderBy(clean_df.dateAdded.desc(),\n",
    "                          clean_df.dateUpdated.desc())\n",
    "recent_df = clean_df.withColumn('date_timestamp',\n",
    "                    unix_timestamp(clean_df.dateAdded.cast('date'))) \\\n",
    "                  .withColumn(\"date_added\", unix_timestamp(clean_df.dateAdded)) \\\n",
    "                  .withColumn(\"date_updated\", unix_timestamp(clean_df.dateUpdated)) \\\n",
    "                  .withColumn('row_number', row_number().over(date_window)) \\\n",
    "                  .filter(col('row_number') == app_constants.Count.RECENT_DATA) \\\n",
    "                  .drop('row_number', 'dateAdded', 'dateUpdated') \\\n",
    "                  .withColumnRenamed('date_added', 'dateAdded') \\\n",
    "                  .withColumnRenamed('date_updated', 'dateUpdated')\n",
    "\n",
    "recent_dict = recent_df.toPandas().to_dict('records')\n",
    "\n",
    "for data in recent_dict:\n",
    "    recent_key = app_constants.KeyMeta.RECENT + app_constants.KeyMeta.JOINER + str(data['date_timestamp'])\n",
    "    db.hmset(recent_key, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_df = clean_df.withColumn('date_timestamp', \\\n",
    "                      unix_timestamp(clean_df.dateAdded.cast('date'))) \\\n",
    "                .groupBy('date_timestamp', 'brand') \\\n",
    "                .agg(count('brand')) \\\n",
    "                .orderBy('date_timestamp', 'count(brand)', ascending=False)\n",
    "\n",
    "count_dict = count_df.toPandas() \\\n",
    "                .groupby('date_timestamp') \\\n",
    "                .apply(lambda x: dict(zip(x['brand'], x['count(brand)']))) \\\n",
    "                .to_dict()\n",
    "\n",
    "for epoch_date, data in count_dict.iteritems():\n",
    "    count_key = app_constants.KeyMeta.COUNT + app_constants.KeyMeta.JOINER + str(epoch_date)\n",
    "    db.hmset(count_key, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "color_window = Window.partitionBy(clean_df.colors) \\\n",
    "                  .orderBy(clean_df.dateAdded.desc(), \\\n",
    "                          clean_df.dateUpdated.desc())\n",
    "\n",
    "color_df = clean_df.select('*', row_number() \\\n",
    "                          .over(color_window) \\\n",
    "                          .alias('row_number')) \\\n",
    "                .filter(col('row_number') <= app_constants.Count.COLOR_DATA) \\\n",
    "                .withColumn('dateAdded', unix_timestamp(clean_df.dateAdded).cast(LongType())) \\\n",
    "                .withColumn('dateUpdated', unix_timestamp(clean_df.dateUpdated).cast(LongType())) \\\n",
    "                .drop('row_number')\n",
    "\n",
    "color_dict = color_df.toPandas() \\\n",
    "                .groupby(['colors']) \\\n",
    "                .apply(lambda x: x.to_dict('records'))\n",
    "\n",
    "for k, v in color_dict.iteritems():\n",
    "    for color in k.split(','):\n",
    "        key = (app_constants.KeyMeta.COLOR + app_constants.KeyMeta.JOINER + color).lower()\n",
    "        if db.exists(key): db.delete(key)\n",
    "\n",
    "for color, data in color_dict.iteritems():\n",
    "    split_colors = color.split(',')\n",
    "\n",
    "    for split_color in split_colors:\n",
    "        color_key = (app_constants.KeyMeta.COLOR + app_constants.KeyMeta.JOINER + split_color).lower()\n",
    "        db.rpush(color_key, *data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
