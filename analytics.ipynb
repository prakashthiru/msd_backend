{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LOAD METHOD 1\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "        .setAppName('analytics') \\\n",
    "        .setMaster(\"local[*]\")\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "# conf = SparkConf().setAppName('analytics')\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Column with dots. Upgrade pyspark > 2.0.0\n",
    "\n",
    "## Loading data as DF\n",
    "context_df = sqlContext.read.csv('data/test.csv', header = True)\n",
    "\n",
    "## Loading data as RDD\n",
    "# rd = sc.textFile('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LOAD METHOD 2\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To execute operations in cluster\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('analytics') \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session_df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"delimiter\", \",\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import redis\n",
    "# db = redis.StrictRedis(host = 'localhost', port = 6379, db = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from walrus import *\n",
    "db = Database(host = 'localhost', port = 6379, db = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_joiner = ':'\n",
    "\n",
    "recent_key_meta = 'recent'\n",
    "count_key_meta = 'count'\n",
    "color_key_meta = 'color'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "required_columns = ['id', 'brand', 'colors', 'dateAdded', 'dateUpdated']\n",
    "min_df = session_df.select(required_columns)\n",
    "\n",
    "# exclude_columns = [col for col in session_df.columns if col not in required_columns]\n",
    "# session_df.drop(*exclude_columns)\n",
    "\n",
    "# # REMOVE DUPLICATE RECORDS AND IF A COLUMN HAS NULL VALUE\n",
    "# clean_df = session_df.dropna() \\\n",
    "#                     .dropDuplicates() \\\n",
    "#                     .select(required_columns)\n",
    "\n",
    "clean_df = min_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# API II - /getBrandsCount\n",
    "# clean_df.withColumn('date_added', clean_df.dateAdded.cast('date')).show()\n",
    "from pyspark.sql.functions import count, unix_timestamp\n",
    "\n",
    "count_df = clean_df.withColumn('date_added', unix_timestamp(clean_df.dateAdded.cast('date'))) \\\n",
    "        .groupBy('date_added', 'brand') \\\n",
    "        .agg(count('brand')) \\\n",
    "        .orderBy('date_added', 'count(brand)', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# available_dates = count_df.select('date_added').distinct().collect()\n",
    "\n",
    "# from pyspark.sql.functions import collect_list, create_map\n",
    "# df_converted = count_df.groupBy('date_added') \\\n",
    "#                         .agg(collect_list(create_map(col('brand'))).alias('mapped')) \\\n",
    "#                         .collect()\n",
    "\n",
    "count_dict = count_df.toPandas() \\\n",
    "        .groupby('date_added') \\\n",
    "        .apply(lambda x: dict(zip(x['brand'], x['count(brand)']))) \\\n",
    "        .to_dict()\n",
    "\n",
    "for epoch_date, data in count_dict.iteritems():\n",
    "    count_key = count_key_meta + key_joiner + str(epoch_date)\n",
    "\n",
    "    if db.exists(count_key):\n",
    "        count_hash = db.get_key(count_key)\n",
    "    else:\n",
    "        count_hash = db.Hash(count_key)\n",
    "    \n",
    "    count_hash.update(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# API III - /getItemsbyColor\n",
    "# clean_df.groupBy(clean_df.colors)\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "color_window = Window.partitionBy(clean_df.colors) \\\n",
    "                    .orderBy(clean_df.dateAdded.desc(), clean_df.dateUpdated.desc()) \\\n",
    "\n",
    "color_df = clean_df.select('*', row_number() \\\n",
    "                            .over(color_window) \\\n",
    "                            .alias('row_number')) \\\n",
    "                    .filter(col('row_number') <= 10) \\\n",
    "                    .drop('row_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "color_dict = color_df.toPandas() \\\n",
    "        .groupby(['colors']) \\\n",
    "        .apply(lambda x: x.to_dict('records'))\n",
    "\n",
    "for color, data in color_dict.iteritems():\n",
    "    split_colors = color.split(',')\n",
    "\n",
    "    for split_color in split_colors:\n",
    "        color_key = (color_key_meta + key_joiner + split_color).lower()\n",
    "\n",
    "        if db.exists(color_key):\n",
    "            color_hash = db.get_key(color_key)\n",
    "        else:\n",
    "            color_hash = db.List(color_key)\n",
    "\n",
    "        color_hash.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# API I - /getRecentItem\n",
    "\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "date_window = Window.partitionBy(clean_df.dateAdded) \\\n",
    "                    .orderBy(clean_df.dateAdded.desc(), clean_df.dateUpdated.desc())\n",
    "\n",
    "# clean_df.withColumn('rank', rank().over(date_window)) \\\n",
    "#         .withColumn('row_number', row_number().over(date_window)) \\\n",
    "#         .filter((col('rank') == 1) & (col('row_number') == 1)) \\\n",
    "#         .show()\n",
    "\n",
    "recent_df = clean_df.withColumn('date_added', unix_timestamp(clean_df.dateAdded.cast('date'))) \\\n",
    "        .withColumn('row_number', row_number().over(date_window)) \\\n",
    "        .filter(col('row_number') == 1) \\\n",
    "        .drop('row_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recent_dict = recent_df.toPandas().to_dict('records')\n",
    "\n",
    "for data in recent_dict:\n",
    "    recent_key = recent_key_meta + key_joiner + str(data['date_added'])\n",
    "\n",
    "    if db.exists(recent_key):\n",
    "        recent_hash = db.get_key(recent_key)\n",
    "    else:\n",
    "        recent_hash = db.Hash(recent_key)\n",
    "    \n",
    "    recent_hash.update(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## NOTES\n",
    "\n",
    "# session_df.count()\n",
    "# len(session_df.columns)\n",
    "\n",
    "# session_df.columns\n",
    "# session_df.printSchema()\n",
    "# session_df.first().dateAdded\n",
    "\n",
    "# session_df.head(5)\n",
    "# session_df.show(1)\n",
    "# session_df.describe('prices_amountMin').show()\n",
    "# session_df.select('brand').show()\n",
    "\n",
    "# session_df.select('brand', 'colors').dropDuplicates().show()\n",
    "# session_df.where(session_df.asins != '').count()\n",
    "# session_df.limit(2).toPandas()\n",
    "\n",
    "## Only spark >= 2.1.0 can load empty strings as null\n",
    "# session_df.dropna(how='any', thresh=None, subset=None).show()\n",
    "# session_df.dropna().show()\n",
    "# session_df.na.drop().count()\n",
    "\n",
    "\n",
    "# from datetime import datetime\n",
    "# from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "# to_date('2017-02-03T22:06:24Z')\n",
    "# to_timestamp(min_df.dateAdded, 'yyyy-MM-dd HH:mm:ss')\n",
    "\n",
    "# from pyspark.sql.types import DateType\n",
    "# min_df.withColumn('date', min_df['dateAdded'].cast('timestamp')).show()\n",
    "\n",
    "# from pyspark.sql.functions import to_date, to_timestamp\n",
    "# min_df.select(to_timestamp(min_df.dateAdded), 'id').show()\n",
    "\n",
    "# from pyspark.sql import Row\n",
    "# from pyspark.sql.functions import count, col, rank, dense_rank, max\n",
    "# min_df.orderBy(min_df.dateAdded.desc()).show()\n",
    "# min_df.groupBy('dateAdded').count().show()\n",
    "# session_df.agg(max('dateAdded')).show()\n",
    "\n",
    "# from pyspark.sql import Window\n",
    "# w = Window.partitionBy('dateAdded')\n",
    "# session_df.withColumn('maxDate', max('dateAdded').over(w)) \\\n",
    "#     .where(col('dateAdded') == col('maxDate')) \\\n",
    "#     .drop('maxDate') \\\n",
    "#     .count()\n",
    "\n",
    "# import datetime\n",
    "# import numpy as np\n",
    "\n",
    "# w = Window.partitionBy(clean_df['dateAdded']).orderBy(clean_df.dateAdded.desc(), clean_df.dateUpdated.desc())\n",
    "\n",
    "# clean_df.select('*', dense_rank().over(w).alias('rank')) \\\n",
    "#   .filter(col('rank') <= 2) \\\n",
    "#   .show(5)\n",
    "\n",
    "# from datetime import datetime\n",
    "# datetime.fromtimestamp(float('1430677800'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
