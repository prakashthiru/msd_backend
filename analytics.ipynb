{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from pyspark.sql import Window, Row\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql.functions import rank, row_number\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from walrus import *\n",
    "\n",
    "import pandas as pd\n",
    "import app_constants\n",
    "import spark_setup\n",
    "import database_setup\n",
    "\n",
    "# Setups\n",
    "db = database_setup.DatabaseSetup.db\n",
    "spark = spark_setup.SparkSetup.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading data as spark DF. Column with dots -> Upgrade pyspark > 2.0.0\n",
    "session_df = spark.read \\\n",
    "                .option(\"delimiter\", \",\") \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .option(\"header\", spark_setup.SparkSetup.data_headers) \\\n",
    "                .csv('data/dump.csv')\n",
    "\n",
    "# Data clean up: REMOVE duplicate records and if required column has NULL value\n",
    "clean_df = session_df.drop_duplicates() \\\n",
    "                    .dropna(subset=app_constants.Columns.REQUIRED) \\\n",
    "                    .select(app_constants.Columns.REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_window = Window.partitionBy(clean_df.dateAdded) \\\n",
    "                    .orderBy(clean_df.dateAdded.desc(),\n",
    "                            clean_df.dateUpdated.desc())\n",
    "\n",
    "recent_df = clean_df.withColumn('date_added',\n",
    "                          unix_timestamp(clean_df.dateAdded.cast('date'))) \\\n",
    "                    .withColumn('row_number', row_number().over(date_window)) \\\n",
    "                    .filter(col('row_number') == app_constants.Count.RECENT_DATA) \\\n",
    "                    .drop('row_number')\n",
    "\n",
    "recent_dict = recent_df.toPandas().to_dict('records')\n",
    "for data in recent_dict:\n",
    "    recent_key = app_constants.KeyMeta.RECENT + app_constants.KeyMeta.JOINER + str(data['date_added'])\n",
    "\n",
    "    if db.exists(recent_key):\n",
    "        recent_hash = db.get_key(recent_key)\n",
    "    else:\n",
    "        recent_hash = db.Hash(recent_key)\n",
    "\n",
    "    recent_hash.update(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_df = clean_df.withColumn('date_added', \\\n",
    "                        unix_timestamp(clean_df.dateAdded.cast('date'))) \\\n",
    "                  .groupBy('date_added', 'brand') \\\n",
    "                  .agg(count('brand')) \\\n",
    "                  .orderBy('date_added', 'count(brand)', ascending=False)\n",
    "\n",
    "count_dict = count_df.toPandas() \\\n",
    "                  .groupby('date_added') \\\n",
    "                  .apply(lambda x: dict(zip(x['brand'], x['count(brand)']))) \\\n",
    "                  .to_dict()\n",
    "\n",
    "for epoch_date, data in count_dict.iteritems():\n",
    "    count_key = app_constants.KeyMeta.COUNT + app_constants.KeyMeta.JOINER + str(epoch_date)\n",
    "\n",
    "    if db.exists(count_key):\n",
    "        count_hash = db.get_key(count_key)\n",
    "    else:\n",
    "        count_hash = db.Hash(count_key)\n",
    "\n",
    "    count_hash.update(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_window = Window.partitionBy(clean_df.colors) \\\n",
    "                    .orderBy(clean_df.dateAdded.desc(), \\\n",
    "                            clean_df.dateUpdated.desc())\n",
    "\n",
    "color_df = clean_df.select('*', row_number() \\\n",
    "                                .over(color_window) \\\n",
    "                                .alias('row_number')) \\\n",
    "                    .filter(col('row_number') <= app_constants.Count.COLOR_DATA) \\\n",
    "                    .drop('row_number')\n",
    "\n",
    "color_dict = color_df.toPandas() \\\n",
    "                    .groupby(['colors']) \\\n",
    "                    .apply(lambda x: x.to_dict('records'))\n",
    "\n",
    "for color, data in color_dict.iteritems():\n",
    "    split_colors = color.split(',')\n",
    "\n",
    "    for split_color in split_colors:\n",
    "        color_key = (app_constants.KeyMeta.COLOR + app_constants.KeyMeta.JOINER + split_color).lower()\n",
    "\n",
    "        if db.exists(color_key):\n",
    "            color_hash = db.get_key(color_key)\n",
    "        else:\n",
    "            color_hash = db.List(color_key)\n",
    "\n",
    "        color_hash.extend(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
